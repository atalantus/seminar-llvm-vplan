% IMPORTANT: for all online references, be sure to specify the full URL *and*
% the access date!

@inproceedings{10.1145/349299.349320,
author = {Larsen, Samuel and Amarasinghe, Saman},
title = {Exploiting superword level parallelism with multimedia instruction sets},
year = {2000},
isbn = {1581131992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/349299.349320},
doi = {10.1145/349299.349320},
booktitle = {Proceedings of the ACM SIGPLAN 2000 Conference on Programming Language Design and Implementation},
pages = {145–156},
numpages = {12},
location = {Vancouver, British Columbia, Canada},
series = {PLDI '00}
}

@inproceedings{10.1145/3519939.3523701,
author = {Chen, Yishen and Mendis, Charith and Amarasinghe, Saman},
title = {All you need is superword-level parallelism: systematic control-flow vectorization with SLP},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523701},
doi = {10.1145/3519939.3523701},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {301–315},
numpages = {15},
keywords = {auto-vectorization, optimization},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@online{vecoptgcc,
  author = {Jakub Jelínek},
  title = {Vectorization optimization in GCC},
  year = {2023},
  month = {12},
  url = {https://developers.redhat.com/articles/2023/12/08/vectorization-optimization-gcc},
  note = {Accessed: 2024-11-04}
}

@online{autovecoptgcc,
  author = {GCC Team},
  title = {Auto-vectorization in GCC},
  year = {2023},
  month = {02},
  url = {https://gcc.gnu.org/projects/tree-ssa/vectorization.html},
  note = {Accessed: 2024-11-04}
}

@online{llvmvplanrv,
  author = {Simon Moll, Sebastian Hack},
  title = {VPlan + Rv: A Proposal},
  year = {2017},
  url = {https://www.youtube.com/watch?v=svMEphbFukw},
  note = {Accessed: 2024-11-11}
}

@article{gccllvmveccomp,
  author = {Klara Modin},
  title = {A comparison of auto-vectorization performance between GCC and LLVM for the RISC-V vector extension},
  school = {KTH School of Electrical Engineering and Computer Science},
  type = {Bachelor's Thesis},
  year = {2024},
  month = {10},
  url = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-354873}
}

@online{llvmvec,
  author = {LLVM Team},
  title = {Auto-Vectorization in LLVM},
  year = {2021},
  month = {07},
  url = {https://llvm.org/docs/Vectorizers.html},
  note = {Accessed: 2024-11-18}
}

@online{llvmvplan,
  author = {LLVM Team},
  title = {Vectorization Plan},
  year = {2024},
  month = {05},
  url = {https://llvm.org/docs/VectorizationPlan.html},
  note = {Accessed: 2024-11-18}
}

@online{llvmvplanupdate,
  author = {Hahn, Florian and Zaks, Ayal},
  title = {VPlan: Status Update and Future Roadmap},
  year = {2023},
  url = {https://llvm.org/devmtg/2023-10/slides/techtalks/Hahn-VPlan-StatusUpdateAndRoadmap.pdf},
  note = {Accessed: 2024-11-19}
}

@online{llvmvplanstate,
  author = {Zaks, Ayal and Rapaport, Gil},
  title = {Vectorizing Loops with VPlan – Current State and Next Steps},
  year = {2017},
  url = {https://llvm.org/devmtg/2017-10/slides/Zaks-Vectorizing%20Loops%20with%20VPlan.pdf},
  note = {Accessed: 2024-11-16}
}

@online{llvmouterloopstatus,
  author = {Hahn, Florian and Guggilla, Satish and Caballero, Diego},
  title = {Outer Loop Vectorization in LLVM: Current Status and Future Plans},
  year = {2018},
  url = {https://llvm.org/devmtg/2018-10/talk-abstracts.html#talk21},
  note = {Accessed: 2024-11-22}
}

@online{llvmpolyhedral,
  author = {Polly Team},
  year = {2017},
  title = {Polly - LLVM Framework for High-Level Loop and Data-Locality Optimizations},
  url = {https://polly.llvm.org/index.html},
  note = {Accessed: 2024-11-11}
}

@misc{taneja2024llmvectorizerllmbasedverifiedloop,
      title={LLM-Vectorizer: LLM-based Verified Loop Vectorizer}, 
      author={Jubi Taneja and Avery Laird and Cong Yan and Madan Musuvathi and Shuvendu K. Lahiri},
      year={2024},
      eprint={2406.04693},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.04693}, 
}

@article{9802745,
  author={Adit, Neil and Sampson, Adrian},
  journal={IEEE Micro}, 
  title={Performance Left on the Table: An Evaluation of Compiler Autovectorization for RISC-V}, 
  year={2022},
  volume={42},
  number={5},
  pages={41-48},
  keywords={Codes;Benchmark testing;Programming;Registers;Program processors;Reduced instruction set computing;Instruction sets},
  doi={10.1109/MM.2022.3184867}}

@article{POHL2020102106,
title = {Vectorization cost modeling for NEON, AVX and SVE},
journal = {Performance Evaluation},
volume = {140-141},
pages = {102106},
year = {2020},
issn = {0166-5316},
doi = {https://doi.org/10.1016/j.peva.2020.102106},
url = {https://www.sciencedirect.com/science/article/pii/S0166531620300262},
author = {Angela Pohl and Biagio Cosenza and Ben Juurlink},
}

@article{10.1155/2021/3264624,
author = {Feng, Jing Ge and He, Ye Ping and Tao, Qiu Ming and Wahid, Fazli},
title = {Evaluation of Compilers’ Capability of Automatic Vectorization Based on Source Code Analysis},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/3264624},
doi = {10.1155/2021/3264624},
abstract = {Automatic vectorization is an important technique for compilers to improve the parallelism of programs. With the widespread usage of SIMD (Single Instruction Multiple Data) extensions in modern processors, automatic vectorization has become a hot topic in the research of compiler techniques. Accurately evaluating the effectiveness of automatic vectorization in typical compilers is quite valuable for compiler optimization and design. This paper evaluates the effectiveness of automatic vectorization, analyzes the limitation of automatic vectorization and the main causes, and improves the automatic vectorization technology. This paper firstly classifies the programs by two main factors: program characteristics and transformation methods. Then, it evaluates the effectiveness of automatic vectorization in three well-known compilers (GCC, LLVM, and ICC, including their multiple versions in recent 5 years) through TSVC (Test Suite for Vectorizing Compilers) benchmark. Furthermore, this paper analyzes the limitation of automatic vectorization based on source code analysis, and introduces the differences between academic research and engineering practice in automatic vectorization and the main causes, Finally, it gives some suggestions as to how to improve automatic vectorization capability.},
journal = {Sci. Program.},
month = nov,
numpages = {15}
}

@article{2023arXiv230201131K,
       author = {{Karuppanan}, Sayinath and {Mirbagher Ajorpaz}, Samira},
        title = "{An Attack on The Speculative Vectorization: Leakage from Higher Dimensional Speculation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security},
         year = 2023,
        month = feb,
          eid = {arXiv:2302.01131},
        pages = {arXiv:2302.01131},
          doi = {10.48550/arXiv.2302.01131},
archivePrefix = {arXiv},
       eprint = {2302.01131},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230201131K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System},
      numpages = {15}
}

@INPROCEEDINGS{4336219,
  author={Shin, Jaewook},
  booktitle={16th International Conference on Parallel Architecture and Compilation Techniques (PACT 2007)}, 
  title={Introducing Control Flow into Vectorized Code}, 
  year={2007},
  volume={},
  number={},
  pages={280-291},
  keywords={Parallel processing;Automatic generation control;Microprocessors;Automatic control;Kernel;Parallel architectures;Mathematics;Computer science;Laboratories;Clocks},
  doi={10.1109/PACT.2007.4336219}
}

@online{llvmveccontrolflow,
  author = {Ashutosh Nema, Anupama Rasale},
  title = {Improving Vectorization for Loops with Control Flow},
  year = {2023},
  url = {https://www.youtube.com/watch?v=mKG0NmGtpbE},
  note = {Accessed: 2024-11-04}
}

@online{llvmextloopvec,
  author = {Saito, Hideki and Intel Vectorizer Team},
  title = {Extending LoopVectorizer towards supporting OpenMP4.5 SIMD and outer loop auto-vectorization},
  year = {2016},
  url = {https://llvm.org/devmtg/2016-11/Slides/Saito-NextLevelLLVMLoopVectorizer.pdf},
  note = {Accessed: 2024-11-18}
}

@online{llvmintrvplan,
  author = {Rapaport, Gil and Zaks, Ayal},
  title = {Introducing VPlan to the Loop Vectorizer},
  year = {2017},
  url = {https://llvm.org/devmtg/2017-03//assets/slides/introducing_vplan_to_the_loop_vectorizer.pdf},
  note = {Accessed: 2024-11-16}
}

@online{intelouterloop,
  author = {Intel},
  title = {Outer Loop Vectorization},
  year = {2019},
  url = {https://www.intel.com/content/www/us/en/developer/articles/technical/outer-loop-vectorization.html},
  note = {Accessed: 2024-11-06}
}

@online{intelvecessen,
  author = {Intel},
  title = {Vectorization Essentials},
  year = {2019},
  url = {https://www.intel.com/content/www/us/en/developer/articles/technical/vectorization-essential.html},
  note = {Accessed: 2024-11-11}
}

@online{llvmboscc,
  author = {Nema, Ashutosh and Rasale, Anupama},
  title = {Improving Vectorization for Loops with Control Flow},
  year = {2023},
  url = {https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/05-EuroLLVM23%20-%20Improving%20Vectorization%20for%20Loops%20with%20Control%20Flow.pdf},
  note = {Accessed: 2024-11-20}
}

@inproceedings{10.5555/1299042.1299055,
author = {Shin, Jaewook},
title = {Introducing Control Flow into Vectorized Code},
year = {2007},
isbn = {0769529445},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Single instruction multiple data (SIMD) functional units are ubiquitous in modern microprocessors. Effective use of these SIMD functional units is essential in achieving the highest possible performance. Automatic generation of SIMD instructions in the presence of control flow is chal- lenging, however, not only because SIMD code is hard to generate in the presence of arbitrarily complex control flow, but also because the SIMD code executing the instructions in all control paths may slow compared to the scalar orig- inal, which may bypass a large portion of the code. One promising technique introduced recently involves inserting branches-on-superword-condition-codes (BOSCCs) to by- pass vector instructions. In this paper, we describe two techniques that improve on the previous approach. First, BOSCCs are generated in a nested fashion so that even BOSCCs themselves can be bypassed by other BOSCCs. Second, we generate all vec_any_* instructions to by- pass even some predicate-defining instructions. We imple- mented these techniques in a vectorizing compiler. On 14 kernels, the compiler achieves distinct speedups, including 1.99X over the previous technique that generates single- level BOSCCs and vec_any_ne only.},
booktitle = {Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques},
pages = {280–291},
numpages = {12},
series = {PACT '07}
}

@inproceedings{10.5555/3615924.3615932,
author = {Paktinatkeleshteri, Rouzbeh and de Carvalho, Jo\~{a}o P. L and Amiri, Ehsan and Amaral, J. Nelson},
title = {Efficient Auto-Vectorization for Control-flow Dependent Loops through Data Permutation},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {74–83},
numpages = {10},
keywords = {SIMD, Scalable-Vector Extensions, Compilers},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@article{10.1007/s11227-022-04359-w,
author = {Praharenka, Wyatt and Pankratz, David and De Carvalho, Jo\~{a}o P. L. and Amiri, Ehsan and Amaral, Jos\'{e} Nelson},
title = {Vectorizing divergent control flow with active-lane consolidation on long-vector architectures},
year = {2022},
issue_date = {Jul 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {10},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-022-04359-w},
doi = {10.1007/s11227-022-04359-w},
journal = {Journal of Supercomputing},
month = jul,
pages = {12553–12588},
numpages = {36},
keywords = {Instruction-set architecture design, Code generation, Control-flow divergence, Scalable vector extension, Vectorization}
}

@inproceedings{DBLP:conf/vldb/BensonER23,
  author       = {Lawrence Benson and
                  Richard Ebeling and
                  Tilmann Rabl},
  title        = {Evaluating {SIMD} Compiler-Intrinsics for Database Systems},
  booktitle    = {Joint Proceedings of Workshops at the 49th International Conference
                  on Very Large Data Bases {(VLDB} 2023), Vancouver, Canada, August
                  28 - September 1, 2023},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {3462},
  publisher    = {CEUR-WS.org},
  year         = {2023},
  url          = {https://ceur-ws.org/Vol-3462/ADMS5.pdf},
  timestamp    = {Thu, 15 Aug 2024 07:54:16 +0200},
  biburl       = {https://dblp.org/rec/conf/vldb/BensonER23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1145/2366231.2337210,
author = {Satish, Nadathur and Kim, Changkyu and Chhugani, Jatin and Saito, Hideki and Krishnaiyer, Rakesh and Smelyanskiy, Mikhail and Girkar, Milind and Dubey, Pradeep},
title = {Can traditional programming bridge the Ninja performance gap for parallel computing applications?},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2366231.2337210},
doi = {10.1145/2366231.2337210},
abstract = {Current processor trends of integrating more cores with wider SIMD units, along with a deeper and complex memory hierarchy, have made it increasingly more challenging to extract performance from applications. It is believed by some that traditional approaches to programming do not apply to these modern processors and hence radical new languages must be discovered. In this paper, we question this thinking and offer evidence in support of traditional programming methods and the performance-vs-programming effort effectiveness of common multi-core processors and upcoming many-core architectures in delivering significant speedup, and close-to-optimal performance for commonly used parallel computing workloads.We first quantify the extent of the "Ninja gap", which is the performance gap between naively written C/C++ code that is parallelism unaware (often serial) and best-optimized code on modern multi-/many-core processors. Using a set of representative throughput computing benchmarks, we show that there is an average Ninja gap of 24X (up to 53X) for a recent 6-core Intel® Core™ i7 X980 Westmere CPU, and that this gap if left unaddressed will inevitably increase. We show how a set of well-known algorithmic changes coupled with advancements in modern compiler technology can bring down the Ninja gap to an average of just 1.3X. These changes typically require low programming effort, as compared to the very high effort in producing Ninja code. We also discuss hardware support for programmability that can reduce the impact of these changes and even further increase programmer productivity. We show equally encouraging results for the upcoming Intel® Many Integrated Core architecture (Intel® MIC) which has more cores and wider SIMD. We thus demonstrate that we can contain the otherwise uncontrolled growth of the Ninja gap and offer a more stable and predictable performance growth over future architectures, offering strong evidence that radical language changes are not required.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {440–451},
numpages = {12}
}

@inproceedings{10.5555/2337159.2337210,
author = {Satish, Nadathur and Kim, Changkyu and Chhugani, Jatin and Saito, Hideki and Krishnaiyer, Rakesh and Smelyanskiy, Mikhail and Girkar, Milind and Dubey, Pradeep},
title = {Can traditional programming bridge the Ninja performance gap for parallel computing applications?},
year = {2012},
isbn = {9781450316422},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current processor trends of integrating more cores with wider SIMD units, along with a deeper and complex memory hierarchy, have made it increasingly more challenging to extract performance from applications. It is believed by some that traditional approaches to programming do not apply to these modern processors and hence radical new languages must be discovered. In this paper, we question this thinking and offer evidence in support of traditional programming methods and the performance-vs-programming effort effectiveness of common multi-core processors and upcoming many-core architectures in delivering significant speedup, and close-to-optimal performance for commonly used parallel computing workloads.We first quantify the extent of the "Ninja gap", which is the performance gap between naively written C/C++ code that is parallelism unaware (often serial) and best-optimized code on modern multi-/many-core processors. Using a set of representative throughput computing benchmarks, we show that there is an average Ninja gap of 24X (up to 53X) for a recent 6-core Intel® Core™ i7 X980 Westmere CPU, and that this gap if left unaddressed will inevitably increase. We show how a set of well-known algorithmic changes coupled with advancements in modern compiler technology can bring down the Ninja gap to an average of just 1.3X. These changes typically require low programming effort, as compared to the very high effort in producing Ninja code. We also discuss hardware support for programmability that can reduce the impact of these changes and even further increase programmer productivity. We show equally encouraging results for the upcoming Intel® Many Integrated Core architecture (Intel® MIC) which has more cores and wider SIMD. We thus demonstrate that we can contain the otherwise uncontrolled growth of the Ninja gap and offer a more stable and predictable performance growth over future architectures, offering strong evidence that radical language changes are not required.},
booktitle = {Proceedings of the 39th Annual International Symposium on Computer Architecture},
pages = {440–451},
numpages = {12},
location = {Portland, Oregon},
series = {ISCA '12}
}

@inproceedings{10.5555/977395.977673,
author = {Lattner, Chris and Adve, Vikram},
title = {LLVM: A Compilation Framework for Lifelong Program Analysis \& Transformation},
year = {2004},
isbn = {0769521029},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization},
pages = {75},
location = {Palo Alto, California},
series = {CGO '04}
}

@article{chinese180,
author = {Feng Jingge, He Yeping, Tao Qiuming},
title = {Automatic Vectorization: Recent Progress and Outlook},
publisher = {通信学报},
year = {2022},
journal = {Journal of Communications},
volume = {43},
number = {3},
eid = {180},
numpages = {15},
pages = {180 - 195},
url = {https://www.infocomm-journal.com/txxb/CN/abstract/article_172304.shtml},
doi = {10.11959/j.issn.1000-436x.2022051}
}

@inproceedings{10.1145/3192366.3192413,
author = {Moll, Simon and Hack, Sebastian},
title = {Partial control-flow linearization},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192413},
doi = {10.1145/3192366.3192413},
abstract = {If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 \% over ICC, GCC and Clang O3.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {543–556},
numpages = {14},
keywords = {SPMD, SIMD, Compiler optimizations},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}



@online{rv,
  author = {Compiler Design Lab - Saarland University},
  title = {RV: A Unified Region Vectorizer for LLVM},
  year = {2024},
  url = {https://github.com/cdl-saarland/rv},
  note = {Accessed: 2024-11-11}
}

@online{llvmouterloop,
  author = {Caballero, Diego and Intel Vectorizer Team},
  title = {Extending LoopVectorize to Support Outer Loop Vectorization Using VPlan},
  year = {2018},
  url = {https://llvm.org/devmtg/2018-04/slides/Caballero-Extending%20LoopVectorize%20to%20Support%20Outer%20Loop%20Vectorization%20Using%20VPlan.pdf},
  note = {Accessed: 2024-11-18}
}

@online{rvproposal,
  author = {Moll, Simon and Hack, Sebastian},
  title = {VPlan + RV: A Proposal},
  year = {2017},
  url = {https://www.llvm.org/devmtg/2017-10/slides/Moll-Vplan.pdf},
  note = {Accessed: 2024-11-19}
}

@online{rvproposaldep,
  author = {Moll, Simon and Klößner, Thorsten and Hack, Sebastian},
  title = {RFC: A new divergence analysis for LLVM},
  year = {2018},
  url = {https://llvm.org/devmtg/2018-04/slides/Moll-A%20new%20divergence%20analysis%20for%20LLVM.pdf},
  note = {Accessed: 2024-11-19}
}

@inproceedings{10.1145/3303117.3306172,
author = {Simon Moll and Shrey Sharma and Matthias Kurtenacker and Sebastian Hack },
title = {Multi-dimensional Vectorization in LLVM},
year = {2019},
isbn = {9781450362917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303117.3306172},
doi = {10.1145/3303117.3306172},
booktitle = {Proceedings of the 5th Workshop on Programming Models for SIMD/Vector Processing},
articleno = {3},
numpages = {8},
keywords = {Compiler, Optimization, SIMD, Tensor, Vectorization},
location = {Washington, DC, USA},
series = {WPMVP'19}
}

@inproceedings{10.1145/1454115.1454119,
author = {Dorit Nuzman and Ayal Zaks},
title = {Outer-loop vectorization: revisited for short SIMD architectures},
year = {2008},
isbn = {9781605582825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454115.1454119},
doi = {10.1145/1454115.1454119},
abstract = {Vectorization has been an important method of using data-level parallelism to accelerate scientific workloads on vector machines such as Cray for the past three decades. In the last decade it has also proven useful for accelerating multi-media and embedded applications on short SIMD architectures such as MMX, SSE and AltiVec. Most of the focus has been directed at innermost loops, effectively executing their iterations concurrently as much as possible. Outer loop vectorization refers to vectorizing a level of a loop nest other than the innermost, which can be beneficial if the outer loop exhibits greater data-level parallelism and locality than the innermost loop. Outer loop vectorization has traditionally been performed by interchanging an outer-loop with the innermost loop, followed by vectorizing it at the innermost position. A more direct unroll-and-jam approach can be used to vectorize an outer-loop without involving loop interchange, which can be especially suitable for short SIMD architectures.In this paper we revisit the method of outer loop vectorization, paying special attention to properties of modern short SIMD architectures. We show that even though current optimizing compilers for such targets do not apply outer-loop vectorization in general, it can provide significant performance improvements over innermost loop vectorization. Our implementation of direct outer-loop vectorization, available in GCC 4.3, achieves speedup factors of 3.13 and 2.77 on average across a set of benchmarks, compared to 1.53 and 1.39 achieved by innermost loop vectorization, when running on a Cell BE SPU and PowerPC970 processors respectively. Moreover, outer-loop vectorization provides new reuse opportunities that can be vital for such short SIMD architectures, including efficient handling of alignment. We present an optimization tapping such opportunities, capable of further boosting the performance obtained by outer-loop vectorization to achieve average speedup factors of 5.26 and 3.64.},
booktitle = {Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques},
pages = {2–11},
numpages = {10},
keywords = {vectorization, subword parallelism, data reuse, SIMD},
location = {Toronto, Ontario, Canada},
series = {PACT '08}
}