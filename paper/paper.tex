\documentclass[sigplan,11pt,nonacm]{acmart}
\settopmatter{printfolios}

\usepackage{booktabs} % For formal tables
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{hyphenat}
\usepackage{todonotes}
\usepackage{flushend}
\usepackage{xurl}
\usepackage[babel]{csquotes}

\begin{document}
\title[State of LLVM's Vectorization Plan]{Utilizing Parallel Workers: LLVM's Vectorization Plan}
\author{Jonas Fritsch}
\affiliation{%
  \institution{Technical University of Munich}
}
\email{jonas.fritsch@tum.de}

\begin{abstract}
  Modern SIMD processors provide various vector registers and ISAs for programmers to utilize. 
  However, manually vectorizing code can be time-consuming, so compilers look to auto-vectorize 
  code for better performance. LLVM, as a widely used compiler toolchain, could only auto-vectorize 
  basic scalar code. Intel proposed a comprehensive refactoring of the underlying system to enable 
  LLVM's auto-vectorization to support more complex optimizations. The suggested Vectorization Plan 
  (VPlan) architecture would be more modular and scalable. Since then, significant efforts have been 
  made to implement this new system. Today, VPlan is already actively involved in vectorizing 
  innermost loops. Other areas, such as outer loop vectorization, are also in development.
  In this paper, we introduce VPlan and summarize its latest development results.
\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\label{sec:introduction}
Modern CPUs are often equipped with multiple different vector registers. These registers are nowadays 
as wide as 512 bits or more, allowing for the processing of multiple data streams at once (SIMD). By 
batching multiple values together in one register, different ISAs like Intel AVX or ARM SVE allow 
the execution of the same instruction for all values in a vector register simultaneously. 
Utilizing this leads to significant performance improvements over the scalar equivalent most of 
the time.

However, manual code vectorization can quickly become time-consuming, especially when
supporting different CPU architectures. Modern compilers aim to automatically transform scalar code
to use vectorization when applicable.

As one of the most widely used compilation frameworks, LLVM~\cite{10.5555/977395.977673} has 
implemented and refined its auto-vectorization over many years. It provides two different 
Vectorizers, one for innermost loops (LoopVectorize) and one for super-word parallelism 
(SLPVectorize)~\cite{llvmvec}.

This system, however, had quite a few limitations, as the loop vectorizer could only handle
innermost loops and neither outer loops, complex control-flow, nor non-inlined functions. 
Additionally, while multiple different vectorizations for the same scalar code 
would be possible, the current vectorizers working directly on the LLVM IR could not model
and compare the costs of such different vectorization approaches.

With these limitations in mind, Intel started an ongoing refactorization effort to migrate LLVM's
auto-vectorization pipeline to utilize a more abstract Vectorization Plan 
(VPlan)~\cite{llvmextloopvec,llvmvplan}. The final goal would be to unite LLVM's auto-vectorization
in a single flexible system capable of optimizing SLPs, and inner and nested loops with complex 
control-flow.

The auto-vectorization pass would create multiple different VPlans based on initial legality 
checks. Each VPlan represents a different vectorization approach, by modeling the
underlying scalar control flow with its own abstracted vectorized version. Different VPlan-to-VPlan
transformations can then optimize these abstracted control-flows based on a cost model.
After that, the best VPlan, and with it, the best vectorization approach is chosen by the cost model.
This VPlan is then executed by going over its own vectorized intermediate representation and
materializing it back into the underlying LLVM IR.

LLVM's VPlan system is currently being integrated into the existing Loop Vectorizer and is
already modeling most inner loop vectorizations and transformations~\cite{llvmvplanupdate}. 
Vectorization for outer loops is also in development and can be enabled by setting 
the \texttt{-enable-\allowbreak vplan-\allowbreak native-\allowbreak path} feature 
flag~\cite{llvmouterloop,llvmouterloopstatus}. 
In the future, the plan will be to merge both of these loop vectorization paths into one. 
Merging the existing SLP Vectorizer with VPlan is also planned.

The rest of this paper is organized as follows. First, we explain different vectorization
strategies and constraints in general and describe the original auto-vectorization
approach in LLVM. Section~\ref{sec:vplan} then introduces the new VPlan system, focusing on
its core VPlan component. Next, we summarize the current state of VPlan as of LLVM version 19.1.4.
Section~\ref{sec:vplanfuture} gives an outlook on future developments regarding VPlan. In 
Section~\ref{sec:relatedwork}, we highlight other approaches to auto-vectorization and 
comparisons of LLVM's auto-vectorization to other compilers such as GCC. 
Concluding, we summarize the intentions of VPlan, its current state, and its potential future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Background}
\label{sec:background}

\subsection{Vectorization Strategies}
\label{back:vec-strat}
In general, vectorization strategies can be separated into different categories:

\paragraph{Inner Loop Vectorization}
Innermost loops are loops whose body's control flow does not contain any more loops.
Vectorization of these loops is more straightforward as the only option to vectorize is over 
the one loop's induction variables.

There are various ways to transform the control flow of a loop to accommodate operating on widened
vector registers. Figure~\ref{fig:inner-loop-vec} shows an example of an if-conversion. Some more
examples can also be found in LLVM's Auto-Vectorization Documentation~\cite{llvmvec}.

\begin{figure}
  \centering
  \includegraphics[width=0.35\textwidth]{images/inner-loop-vec.png}
  \caption{A code snippet of a simple loop that would be vectorized over the variable \texttt{i}. 
  The conditional branch would be flattened into a masked addition. The mask would be generated 
  based on \texttt{z[i]}.}
  \label{fig:inner-loop-vec}
\end{figure}

\paragraph{Super-Word Parallelism (SLP) Vectorization}
SLP Vectorization vectorizes similar independent sequential instructions. This can often be seen 
after a loop has been partially or fully unrolled. The resulting similar scalar 
instructions might then be vectorized later by an SLP vectorizer.

\paragraph{Outer Loop Vectorizaton}
Outer loop vectorization focuses on control flow with nested loops. Such control flow often presents
more challenges for auto-vectorization, as one could vectorize over the outer loop,
inner loop, or a mix of both, representing a hybrid/multi-dimensional vectorization approach.
Finding the best legal vectorization can, therefore, be more difficult.

Figure~\ref{fig:outer-loop-vec} shows a nested loop code snippet where vectorization over the
outer loop would be beneficial.

\begin{figure}
  \centering
  \includegraphics[width=0.39\textwidth]{images/outer-loop-vec.png}
  \caption{A code snippet of a nested loop where it would be beneficial to vectorize over the 
  outer loop. The inner loop has a trip count that is too low to vectorize (6). Additionally,
  the memory access pattern of \texttt{y} would be consecutive compared to scattered when
  vectorizing over \texttt{i} instead of \texttt{j}.}
  \label{fig:outer-loop-vec}
\end{figure}

\paragraph{Function Vectorization}
Function vectorization refers to the vectorization of entire functions. A loop might contain a
non-inlined function call, passing data related to the iteration variable. During auto-vectorization, 
this call could be replaced with a call to a newly constructed function with the same semantic 
control-flow but operating on multiple values at once by utilizing vector registers.
 
\subsection{Vectorization Legality}
\label{sec:vec-legal}
Not all loops can be vectorized. There are various factors that can ultimately hinder any
vectorization attempts.

Some of the most common vectorization barriers are data dependencies. As the vectorized version
of a code executes for Vectorization Factor (VF) data streams simultaneously, it inherently changes
the order of operations. To guarantee that the semantics of the code do not change, all
operations on these VF data streams must, generally, be independent. The simplest case
where this is not provided is when the $i$-th loop iteration depends on any of the 
previous iterations up to the $(i-\text{VF})$-th.

While explicit data dependencies are easier to see, a simple loop, as in 
Figure~\ref{fig:inner-loop-vec}, could carry an implicit dependency in line 3 if
the \texttt{x} and \texttt{y} point to overlapping memory regions (pointer aliasing).

Not all data dependencies directly hinder any vectorization~\cite{datadepvec}. An exception can, for example, 
sometimes be made for reduction idioms, as seen in the inner loop in 
Figure~\ref{fig:outer-loop-vec}, where \texttt{sum} would be a reduction variable.

Uncountable loops, meaning loops where the total iteration count cannot be determined before the 
loop is executed, can often not be vectorized. This is because their exit condition usually depends on 
the loop's body itself, making it impossible to predetermine how many iterations could be processed together 
before having to check the exit condition again.

Finally, some instructions do generally not have a direct vectorized 
instruction equivalent. For example Integer division, commonly implemented using the 
Newton-Raphson method, is already a complex and iterative operation 
on modern CPUs. Applying this algorithm 
to multiple data elements in a single vector register would result in even more implementation complexity 
and is thus considered not worthwhile in most cases.
Workarounds for some specialized cases do exist~\cite{vecintdiv}.

\subsection{Vectorization Costs}
While vectorizing a given control flow may not be legal, the vectorized code can sometimes also perform 
worse than its scalar equivalent.

Some control flow transformations that seem beneficial at compile time might reveal a performance
bottleneck at runtime. Considering the code snippet from Figure~\ref{fig:inner-loop-vec}, it might
be that almost all \texttt{z[i]} evaluate to false at runtime. The branch predictor of a CPU
would then be able to guess the correct branch in the scalar code. The vectorized version would
always access the memory at \texttt{y[i]} only for the addition to be masked away. Different 
approaches to avoid such runtime penalties are branch-on-superword-condition code 
(BOSCC)~\cite{10.5555/1299042.1299055,llvmboscc} or active-lane 
consolidation (ALC)~\cite{10.1007/s11227-022-04359-w,10.5555/3615924.3615932}.

The actual performance of a vector instruction also heavily depends on the ISA and its 
hardware implementation. 
For example, gather, scatter, 
shuffle, and permutation instructions are typically considered slow. Control flow with many 
non-adjacent memory accesses or 
operations that combine different vector elements from the same register (horizontal reductions) can thus 
decrease overall performance.

Additionally, the vectorized code size will always be larger than the scalar equivalent. This is
due to many factors: (1) The transformed control flow often includes more instructions.
(2) As the loop trip count might not be a perfect multiple of the vectorization factor, a scalar
epilogue/tail of the loop might be used to handle the remaining loop iterations. (3) Various runtime
checks might be inserted throughout the vectorized control flow to handle, for example, pointers 
to overlapping memory regions, in which case the scalar version of the loop must be executed.

\subsection{Auto-Vectorization in LLVM}
LLVM implements two different vectorizers for auto-vectorization, LoopVectorize (LV)
and SLPVectorize~\cite{llvmvec,llvmhistorystate}. Auto-Vectorization is part of LLVM's 
middle-end and runs after module simplification.
These vectorizers take scalar LLVM IR and transform it into optimized vectorized 
LLVM IR.

SLPVectorize is a SLP vectorizer working on straight-line code (non-loops). It works by analyzing
the code bottom-up, keeping track of dependencies and grouping similar independent scalar instructions
together if possible. Based on a cost model SLPVectorize then decides to vectorize these groups.

LoopVectorize was designed to focus only on vectorization of innermost loops~\cite{llvmintrvplan}. 
When starting with an initial scalable loop in IR, it would first run a legality phase to check 
whether and how it is legal to vectorize the loop. This phase would produce different artifacts, 
such as if 
and what runtime aliasing checks would be required. Then, the cost model ran next, creating more 
cost-based artifacts based on the underlying scalar IR and the legality artifacts. 
Lastly, the transformation phase would vectorize 
the IR in one go based on all the previously produced artifacts.

However, this rigid flow of LoopVectorize presented a challenge. Extending LV to include outer loop 
and nested loop vectorizations would have been difficult, as LV was designed to flatten a
loop's control flow into a single basic block. Producing and considering more artifacts when trying to 
transform the scalar loop into a vectorized all at once also did not scale well.

As a potential solution, Intel proposed a long-term effort to refactor LLVM's whole auto-vectorization
pipeline to use a more modular and scalable system called Vectorization Plan (VPlan)~\cite{llvmextloopvec}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{LLVM's Vectorization Plan}
\label{sec:vplan}

Intel's initial VPlan proposal aimed to unite any 
auto-vectorization strategy (SLP, Inner/Outer-Loop, Whole-Function) into a 
single flexible system~\cite{llvmextloopvec,llvmintrvplan,llvmvplanstate}. 
This system would use VPlan objects to represent different 
vectorization approaches, abstracting away from the underlying scalar loop IR using its own 
hierarchical control flow graph (H-CFG). This allows iteratively exploring different 
optimization decisions by performing transformations on this graph without modifying the 
underlying LLVM IR.
Each VPlan could calculate its cost and execute itself, materializing its vectorization 
approach back into the LLVM IR.

This planned system is shown in Figure~\ref{fig:vplan-future}. At first, the Legality 
Phase would check if the scalar code would be legal to vectorize at all, and if so, 
initial VPlans would be constructed. In the following Planning Phase, these initial 
VPlans would be optimized using various VPlan-to-VPlan transformations~\cite{llvmouterloopstatus}. 
The VPlan's cost model would supervise those transformations. Finally, the best VPlan 
would be chosen and executed, modifying the underlying IR.

\begin{figure}
  \centering
  \includegraphics[width=0.475\textwidth]{images/vplan-future.png}
  \caption{The planned future architecture for auto-vectorization in LLVM. The 
  optimization step in phase 2 would iteratively apply VPlan-to-VPlan transformations.}
  \label{fig:vplan-future}
\end{figure}

\subsection{VPlan Structure}
Before explaining how far this system has been implemented in LLVM, we will review
the different elements of a \textit{VPlan} in more detail.

The purpose of a single VPlan object is to model a candidate for vectorization. 
Such a single candidate can potentially represent multiple different vectorized loops. 
For example, one VPlan might model a general vectorization approach for various 
Vectorization Factors (VFs) and Unroll Factors (UFs).

\paragraph{H-CFG}
\emergencystretch 2em

This is achieved by introducing a new layer of abstraction from the underlying loop's scalar IR.
Each VPlan stores its own Hierarchical-CFG (H-CFG) modeling its vectorization candidate. This 
control-flow graph and its components are the main elements of a single VPlan. It is hierarchical
in that a single node of this graph can be a complete subgraph, representing more complex nested
behavior.
Specifically, any node of this H-CFG is one of three types:

\begin{itemize}
  \item \texttt{VPBasicBlock} stores a list of \texttt{VPRecipes} similar to 
  the LLVM IR's \texttt{BasicBlock} holding a list of IR Instructions.
  \item \texttt{VPRegionBlock} represents a subgraph with a single entry and exit node. 
  A \texttt{VPRegionBlock} can be marked to replicate its subgraph
  \texttt{VF~*~UF} times when executing. This is useful for modeling scalarized or predicated
  instructions within the final vectorized loop body.
  \item \texttt{VPIRBasicBlock} is a special \texttt{VPBasicBlock} that wraps an underlying
  IR \texttt{BasicBlock}. These nodes help model the scalar parts around the 
  final vectorized loops, such as the loop preheader and the epilogue loop.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=0.475\textwidth]{images/inner-loop-scalar-loop-ir-color.png}
  \caption{The scalar IR for the main loop section of the conditional loop depicted in
  Figure~\ref{fig:inner-loop-vec}. Note that \texttt{getelementptr inbounds}
  was abbreviated to \texttt{getelmptr inbs}.}
  \label{fig:inner-loop-scalar-ir}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.475\textwidth]{images/inner-loop-vplan-hcfg-loop-body-color.png}
  \caption{The vectorized loop part of a VPlan's hierarchical CFG modeling the 
  conditional loop depicted in Figure~\ref{fig:inner-loop-vec}. 
  Surrounding nodes were omitted for clarity.
  Some predefined values are
  \texttt{vp<\%0>} = \texttt{VF * UF}; \texttt{vp<\%1>} = vector trip count; \texttt{vp<\%2>} = original trip count.}
  \label{fig:inner-loop-vplan-hcfg-body}
\end{figure}

Figure~\ref{fig:inner-loop-scalar-ir} shows the loop part of the scalar IR for the code example
of Figure~\ref{fig:inner-loop-vec}. Comparatively, Figure~\ref{fig:inner-loop-vplan-hcfg-body}
shows the equivalent vector loop part of a VPlan's H-CFG that models the vectorization approach described
in Figure~\ref{fig:inner-loop-vec}. Note that the complete H-CFG of this VPlan consists of many more nodes 
to handle legality checks (coming before the vectorized loop) or the entire scalar epilogue loop 
(coming after the vectorized loop).

The H-CFG in Figure~\ref{fig:inner-loop-vplan-hcfg-body} consists of 3 nodes on the highest layer. 
\texttt{vector.preheader} and \texttt{middle.block} 
are simple \texttt{VPBasicBlocks} surrounding the main \texttt{vector loop} node. The \texttt{vector loop} node 
is a \texttt{VPRegionBlock} that does 
not replicate its contents (indicated by \texttt{<x1>}; otherwise, it would be \texttt{<xVFxUF>}).

Inside the \texttt{vector loop} region is only a single \texttt{VPBasicBlock} node \texttt{vector.body}. This block 
models the entire vectorized loop via \texttt{VPRecipes}.

\paragraph{Recipes}
\emergencystretch 2em

\texttt{VPRecipes} can be seen as instructions of an intermediate representation specialized for auto-vectorization. 
A single \texttt{VPRecipe} inside a VPlan is based on zero or more LLVM IR instructions 
of the underlying scalar IR. These underlying IR instructions are called \textit{ingredients}. 
Each \texttt{VPRecipe} will materialize back into one or more LLVM IR instructions when it is executed.

There are currently 40 different final types of \texttt{VPRecipes}. The types used 
within Figure~\ref{fig:inner-loop-vplan-hcfg-body} and the most important ones are:
\begin{itemize}
  \item \texttt{VPReplicateRecipe} will replicate one IR instruction either once (\texttt{CLONE})
  or \texttt{VF * UF} times (\texttt{REPLICATE}) into the vectorized IR when executed.
  \item \texttt{VPWidenRecipe} will \texttt{WIDEN} one IR instruction into a
  vector-equivalent when executed.
  \item \texttt{VPInstruction} has no underlying IR instruction it is based on and is used 
  to model newly needed instructions for the vectorized IR. When executed it will 
  \texttt{EMIT} one or more new IR instructions.
  \item \texttt{VPHeaderPHIRecipe} is a sub-type that models any induction, such as first-order 
  recurrence, pointer induction, or reduction as a phi node.
  \item \texttt{VPCanonicalIVPHIRecipe} derives from \texttt{VPHeaderPHIRecipe} and represents a 
  \texttt{CANONICAL-INDUCTION} variable via a \textit{scalar} value.
  \item \texttt{VPScalarIVStepsRecipe} models the individual offsets (\texttt{SCALAR-STEPS}) per 
  vector-lane from a common Integer or Floating-Point induction value.
\end{itemize}

Most of the \texttt{VPRecipes} in Figure~\ref{fig:inner-loop-vplan-hcfg-body} have capitalized 
\texttt{HINTS} indicating their type. Additionally IR instructions/\texttt{VPRecipes} of the same color 
in Figure~\ref{fig:inner-loop-scalar-ir} and Figure~\ref{fig:inner-loop-vplan-hcfg-body}
are related in their semantical effect. 

The only \texttt{VPRecipes} in our example VPlan H-CFG that do not semantically model any 
instructions found in the scalar IR are from the \texttt{middle.block} (purple). 
This block decides if the scalar epilogue loop is still executed after the vectorized loop by 
comparing the vector trip count to the original loop trip count. This additional vector trip count is 
needed as the original loop trip count might not be a perfect multiple of \texttt{VF * UF}, the number of elements 
processed in a single vector loop iteration.

The red recipes generally handle the induction variable \texttt{i} and the termination of the 
vector loop. \texttt{vp<\%3>} is this loop's scalar induction variable, which goes from 0 to 
\texttt{vp<\%1>}, the vector trip count. The per vector-lane offset's step size is 1 (\texttt{ir<1>}), 
meaning that each 
vector-lane offset from the base induction variable (\texttt{vp<\%3>}) is one greater than the previous lane. 
Thus the memory is accessed consecutively.

The blue recipes model the condition guarding the addition. \texttt{vp<\%6>} holds the conditional mask 
and is used for the guarded memory operations.

Finally, the green recipes represent the addition. First, the data from both \texttt{y} and \texttt{x} 
gets loaded, then added together, and then stored back into \texttt{x}.

\paragraph{Transformations}
Another main benefit of the modular VPlan system is that most vector-optimization cases can 
be directly implemented as standalone VPlan-to-VPlan transformations. A VPlan-to-VPlan 
transformation is a static method that either transforms a given VPlan or creates a new 
VPlan based on a given VPlan. The analysis for these transformations can also be done 
directly on the VPlan via its \texttt{VPRecipes}.

Such transformations include for example:
\begin{itemize}
  \item Simplifying \texttt{VPRecipes} by removing blend operations with only one unique input, removing 
  redundant type casts, and simplifying logical patterns.
  \item Removing dead \texttt{VPRecipes} whose produced values have no users consuming them.
  \item Moving loop-invariant \texttt{VPRecipes} out of the vector loop region. A \texttt{VPRecipe} is 
  considered loop-invariant if it does not have any side effects, does not read from memory, is not a 
  phi node, and all its operands are defined outside of the vector loop region.
\end{itemize}

\section{State of VPlan (LLVM 19.1.4)}
In its current state, VPlan is already deeply integrated into LoopVectorize (LV) and is
used to model, optimize, and execute vectorization of innermost loops~\cite{llvmvplan,llvmintrvplan,llvmvplanupdate}. 
While outer loop vectorization via VPlan has already seen some development, any 
outer loop vectorization features are still experimental and must be explicitly enabled 
by passing the \texttt{-enable-\allowbreak vplan-\allowbreak native-\allowbreak path} 
feature flag~\cite{llvmouterloop,llvmouterloopstatus}.

To better understand how VPlan is already being used in LLVM, we will now follow along the 
\texttt{LoopVectorizePass} and summarize its key components.

\subsection{Phase 1: Legality}
The main part of this first phase is checking whether it is even legal to vectorize a given loop. 
In addition to the general vectorization legality concepts mentioned in 
Section~\ref{sec:vec-legal}, LoopVectorize has a several extra conditions to check.

Some of the most important legality checks done are:
\begin{itemize}
  \item The scalar loop's control flow must not contain indirect branches or multiple back-edges.
  An indirect branch is a branch that jumps to a value evaluated at runtime. A back-edge in a loop 
  is an edge from the loop's body back to its start. 
  \item Loops with non-linear control flow must be if-convertible. This means any branch or switch 
  statement must not exit the loop. All instructions of such a predicated block can be executed speculatively 
  (e.g. via masking)
  \item Loop-carried dependencies must be vectorizable (see Section~\ref{sec:vec-legal}). In LV 
  this means explicitly that such a value must \textit{at least} either be an induction variable, a reduction 
  variable, or a fixed-order recurrence.
  \item The loop must not contain function calls outside of standard math library calls.
  \item The number of runtime checks to guarantee no pointers used by recurrences overlap must not be too high.
\end{itemize}

Additionally, during this phase, all recurrences of the loop, such as reductions or inductions, are analyzed,
and their phi nodes are stored for the later initialization of VPlans.

\subsection{Phase 2: Planning}
After the Legality Phase has analyzed the scalar loop and it was deemed legal to vectorize, the 
\texttt{LoopVectorizationCostModel} and \texttt{LoopVectorizationPlanner} are initialized.
The latter one then constructs the initial VPlans for different vectorization factors based on the properties 
gathered during the legal phase
and the cost model initialization. These properties are, for example, recurrence variables or the maximum viable 
vectorization factor.

A VPlan is initialized by creating the H-CFG skeleton building blocks like preheaders, a 
region block for the vector loop, a middle block with the potential runtime check for the scalar 
epilogue loop, and a block for the scalar epilogue loop itself. Then the scalar LLVM IR 
is iterated in topological order, and each scalar IR instruction is transformed into a \texttt{VPRecipe}.
After that the original scalar loop is represented by a general vectorization approach. Lastly, several VPlan 
transformations are executed, transforming and optimizing the VPlan's H-CFG and its \texttt{VPRecipes}.

After all the initial VPlans have been optimized, the best vectorization factor is computed. This is done 
by comparing all generated VPlans for all their possible VFs and choosing the most profitable one. If 
this step computes an optimal VF of $1$, any vectorization is not profitable compared to the 
scalar version.

\paragraph{Cost Model}
Currently, LV and VPlan still mostly rely on the old cost model. This legacy cost model derives costs 
based on the scalar LLVM IR instructions or, if applicable, their direct vectorized equivalent.
This is suboptimal for VPlan as VPlans work on \texttt{VPRecipes} abstracted away from the underlying 
IR. These costs per instruction are then generally lookup tables influenced by the chosen target 
architecture.

A remainder of the old \texttt{LoopVectorize} implementation before VPlan is the 
before-mentioned \texttt{LoopVectorizationCostModel}. It is a single object that computes costs based on the scalar loop 
IR without any relation to different VPlans. For example, it stores the instructions that will remain scalar 
after vectorization or the LLVM values for which the cost can 
be ignored. The latter includes, for instance, loop invariant operations that would be hoisted out of the loop 
or trivially dead instructions.

Additionally, each \texttt{VPRecipe} has a function to calculate its own cost. For most \texttt{VPRecipes} 
that reference one or more LLVM IR ingredients, these are then used to calculate the cost of the recipe.
On the other hand, \texttt{VPRecipes} that do not directly model an underlying LLVM scalar IR instruction, 
such as \texttt{VPInstruction}, can generally not compute their own cost yet, thus returning a cost of zero 
for now.

For simple innermost loops with few vectorization approaches,
relying on the old cost model might not have too much of a performance impact. 
However, once more VPlan transformations and complex
outer/hybrid loop vectorization are implemented, this new cost model would be an important
detail of the VPlan system to reliably choose the most optimal vectorization candidate.

\subsection{Phase 3: Execution}
After the best VPlan has been chosen, it is executed.
For this VPlan, some final transformations are run depending on a specifically chosen VF and UF.
Then a vectorized code skeleton is created in the IR, and the VPlan executed. 
The H-CFG of the VPlan is walked, and each node executed, which in turn executes all 
of its \texttt{VPRecipes} in order. Each \texttt{VPRecipe} produces its respective output IR.

A \texttt{VPTransformState} object is passed along to store information 
needed for generating the output IR, such as the chosen vectorization factor or generated values that were 
used by later \texttt{VPRecipes}.

\begin{figure}
  \centering
  \includegraphics[width=0.35\textwidth]{images/inner-loop-vec-loop-cfg-color.png}
  \caption{The generalized CFG for a vectorized loop with trip count \texttt{N}, 
  iteration variable \texttt{i}, Vectorization Factor \texttt{VF}, 
  and Unroll Factor \texttt{UF} in LLVM. A VPlan would so far explicitly model the red-highlighted 
  region in its H-CFG. If a node branches, the condition is stated in the body, and the two possible 
  path edges are marked with T (True) and F (False).}
  \label{fig:inner-loop-vec-cfg}
\end{figure}

While Figure~\ref{fig:inner-loop-vec-cfg} shows the general CFG of a vectorized loop in LLVM, some 
of its nodes can also be omitted in special cases.

For example, if all the pointers used inside the function are marked as \texttt{noalias}, the compiler 
is guaranteed that they will not overlap and the \texttt{vector.memcheck} block will be omitted.
While the C language and most modern C\texttt{++} compilers have an explicit \texttt{restrict} keyword for this,
Rust automatically applies the \texttt{noalias} attribute to safe references when compiling via LLVM.

Additionally, before the initial VPlans are constructed at the beginning of Phase 2, it is also checked if 
the scalar epilogue loop could be folded into the vector loop itself by increasing the vector 
trip count over the actual loop trip count and masking all operations. This tail-loop folding approach in LLVM 
has not yet been enabled by default.

Otherwise, if the newly created scalar epilogue loop could be vectorized, it would be in a second loop-vectorize 
pass by, again, creating VPlans, finding the best one, and executing it.

\section{Future of VPlan}
\label{sec:vplanfuture}

Many of the VPlan internals and surrounding systems are still in active development. To stay up-to-date 
with the latest developments regarding auto-vectorization in LLVM, one can follow the 
community forum\footnote{\url{https://discourse.llvm.org/c/ir-optimizations/loop-optimizations/62}} or
attend the monthly online 
meetings\footnote{\url{https://discourse.llvm.org/t/monthly-vectorizer-online-sync-up/78978/3}}.

\subsection{VPlan and Integration with LV}
Setting up and integrating the VPlan structure into LV has seen tremendous effort over the last 
few years. Florian Hahn presented future directions for 
VPlan~\cite{llvmvplanupdate}, which mostly revolve around transforming more LV-based decisions into
explicit VPlan-to-VPlan transformations. Additionally, the legacy LV cost model still influences 
many cost-related decisions. However, the transition to a VPlan-based cost model has been 
started. This new model would derive costs based on \texttt{VPRecipes} directly instead of the 
abstracted scalar IR instructions. Besides that, the community also focuses on enabling tail-loop folding by
default, which would lead to a significant performance increase in some instances.

\subsection{Outer Loop Vectorization}
Development around outer loop vectorization has slowed as the focus shifted to VPlan internals 
and integration into LV. The inner and outer loop paths are still separated, and the 
experimental \texttt{-enable-\allowbreak vplan-\allowbreak native-\allowbreak path} flag must be 
set to activate any outer 
loop vectorization. Further development of the outer loop path and eventually merging it 
with the innermost loop path is planned.

\subsection{Merging with SLP Vectorize}
Merging the existing SLP Vectorizer with the current VPlan infrastructure is in the early planning phase.

\subsection{Function Vectorization}
There seems to be currently no active development on whole-function vectorization in the
llvm-project's upstream.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work}
\label{sec:relatedwork}
\subsection{Region Vectorizers}
Next to the main llvm-projects upstream, many projects are using or extending the LLVM toolchain.

One such project that tries to enable auto-vectorization support for inner/outer loops and 
whole-functions is the region vectorizer (RV) from the University Saarland~\cite{rv}.
The idea is that all the different vectorization strategies shown in Section~\ref{back:vec-strat}
can be generalized into a single vectorization approach.

For example, function vectorization can be transformed into loop vectorization by creating a loop 
around the function's body and vectorizing it and vice versa. 
The same applies to SLP Vectorization and loop vectorization by unrolling a loop.

RV aims to have such a generalized auto-vectorization by vectorizing over \textit{regions}.
A region is defined as any control flow with a single entry and one or more non-divergent exits.
Non-divergent exits mean that control flow must not diverge to different exits starting from the 
same entry.
Such a region can then be, for example, an inner/outer loop, function, or SLP code~\cite{rvintro,rvproposal}.

To achieve this, RV implements partial control flow linearization~\cite{Moll_2021,10.1145/3192366.3192413}
and a divergence analysis that can handle unstructured CFGs. There has been an effort to partially
implement this control flow linearization and divergence analysis into LLVM's 
upstream~\cite{rvproposal,rvproposaldep}.

In a later paper, Moll et al. further generalized this region vectorizer to be able to vectorize 
nested loops in a hybrid/multi-dimensional way, pushing optimization capabilities even 
further~\cite{10.1145/3303117.3306172}. 

\subsection{Comparison: GCC and LLVM}
As another popular compiler, GCC has implemented and consistently improved its auto-vectorization 
over the years~\cite{autovecoptgcc,gccvecsum}. Compared to LLVM, GCC also supports 
vectorization of outer loops and tail-loop folding by default.

In recent years, different research has been done about how compilers, especially GCC and 
LLVM, compare in terms of auto-vectorization.

Feng et al. compared multiple versions of GCC, LLVM, and Intel's ICC compiler in auto-vectorizing the 
TSVC benchmark set targeting x86-64~\cite{feng2021evaluation}. They found that in terms of compilation time, GCC 9
has much less overhead ($\approx4\%$) when auto-vectorization is enabled compared to 
LLVM 9 ($\approx24\%$). In both cases, LLVM 9 takes twice as long to compile than GCC 9. 
ICC 19 sits right between GCC 9 and LLVM 9 regarding compilation time.

While in terms of runtime, GCC 9 and LLVM 9 take around 
the same time with auto-vectorization disabled, enabling it improves GCC 9's performance by around $44\%$, 
while for LLVM 9 it is only $12\%$. ICC 19 has a better runtime than GCC 9 and LLVM 9 in both the 
non-vectorized and the auto-vectorized case.

The authors also investigated the number of successfully auto-vectorized code routines inside the TSVC 
benchmark divided into different vectorization categories. Out of a total of 151 different routines, GCC 9 
vectorized 68, LLVM 9 vectorized 49, and ICC 19 did 98.

Feng et al. concluded that auto-vectorizers were mostly limited by analyzing capabilities 
needed to reason about specific optimization approaches and inaccurate cost models.

While LLVM 9 performs poorly in this paper, it is important to notice that at the time of LLVM 9, VPlan was relatively 
early into development ($\approx3$ years) and was still missing 
support for a lot of commonly used vectorization idioms such as reductions, for example.

In a more recent auto-vectorization evaluation comparing GCC 14 with LLVM 17 on the similar TSVC 2 
benchmark targeting RISC-V, the author investigated the produced vectorized code 
and drew comparisons to similar papers~\cite{gccllvmveccomp}. The SVE and RISC-V 
vector extensions are  
vector-length agnostic, 
meaning that, unlike SSE or AVX, vector instructions are not tied to specific vector register sizes. 
This makes the code more compatible and scalable across different underlying hardware. 

In the paper, the author concludes that the achieved performance uplifts by auto-vectorization are very 
similar between GCC 14 and LLVM 17, with GCC only being marginally better. LLVM 17 tends to favor 
generating vector code targeting specific vector sizes. However, it is unclear whether this has an 
actual drawback.

Finally, as mentioned before, the LLVM auto-vectorization community is currently working on 
enabling tail-loop folding by default after noticing a $\approx24\%$ performance difference between the latest 
versions of GCC and LLVM in a SPEC CPU 2017 benchmark targeting RISC-V~\cite{riscvtailfolding}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Summary and Future Work}
\label{sec:summary}
Compilers have been using auto-vectorization to help developers optimize their code 
across platforms by making the best possible use of a target's SIMD capabilities. Over the years, 
LLVM's original auto-vectorization system evolved into an inflexible pipeline, making it hard 
to adapt to new features like outer loop vectorization.

As a solution, Intel proposed a new modular system called VPlan.
By now, VPlan has been in active development for several years. 
This system would use multiple VPlan objects to efficiently 
model and explore different vectorization approaches simultaneously.
It adds another layer of abstraction by using \texttt{VPRecipes} to model 
vectorization-specific instructions over the underlying LLVM IR. 
This allows for efficiently exploring multiple optimization decisions simultaneously, 
keeping the underlying implementation flexible and modular. In the end, a cost model currently 
working based on LLVM IR instead of \texttt{VPRecipes} is used to compare different VPlans 
and choose the most optimal one. Finally, this chosen VPlan is executed, materializing its \texttt{VPRecipes} 
back into LLVM IR.

While inner loop vectorization is already implemented, support for outer loops is still experimental. 
In the near future, development on outer loop vectorization will continue, and merging 
it with the LoopVectorize VPlan path is planned. Besides that, development is currently focusing on enabling 
tail-loop folding by default.

Active development of other approaches, such as SLP and whole-function vectorization based on the VPlan 
architecture, has yet to be started. Lastly, the initially planned cost model which enabled a standalone 
VPlan to calculate its own cost has not been implemented yet but could become a vital system in the future 
of VPlan.

To help evaluate the idea and current state of VPlan, it might be interesting to benchmark and 
compare it against different auto-vectorization systems, 
such as LLVM before VPlan, 
GCC, ICC, and region vectorizers~\cite{Moll_2021,10.1145/3303117.3306172,rv}. Interesting 
metrics to 
compare would be compile time, performance and size of the produced code, differences in the chosen 
vectorization approaches, differences in legality checks that lead to loops not being vectorized, 
correctness, and stability. These metrics can then differ based on the target architecture.

Various suitable benchmark sets exist that have also been used for similar 
comparisons, such as TSVC 2 or SPEC CPU 2017.

In conclusion, the new VPlan architecture has already transformed LLVM's previously rigid auto-vectorization 
pipeline into a much more modular and flexible system. While some important features are still missing, the 
LLVM community has continuously improved VPlan over the last few years and will continue to do so.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper} % read paper.bib file

\end{document}
